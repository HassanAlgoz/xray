{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_directml\n",
    "# select the GPU device if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch_directml.is_available():\n",
    "    device = torch_directml.device(torch_directml.default_device())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = 0.1307\n",
    "data_std = 0.3081\n",
    "transform = transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x1 = self.data[index]\n",
    "        x2 = random.choice(self.data)\n",
    "        return x1, x2, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split and group training data per class\n",
    "trainsets = []\n",
    "\n",
    "dataset = MNIST(root='data', train=True, download=True, transform=transform)\n",
    "for digit in range(10):\n",
    "    data = torch.utils.data.dataset.Subset(mnist, torch.nonzero(mnist.targets == digit).view(-1)).dataset.data\n",
    "    trainset = PairDataset(data, digit)\n",
    "    trainsets.append(trainset)\n",
    "\n",
    "testset = MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\n",
    "    'Zero',   \n",
    "    'One',    \n",
    "    'Two',   \n",
    "    'Three',  \n",
    "    'Four',    \n",
    "    'Five', \n",
    "    'Six',  \n",
    "    'Seven',  \n",
    "    'Eight',   \n",
    "    'Nine'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(DataLoader(trainsets[0]))\n",
    "data = next(dataiter)\n",
    "im1, im2, labels = data\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(im1))\n",
    "imshow(torchvision.utils.make_grid(im2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_size, d_model, patch_size, nhead, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        \n",
    "        self.embed = PatchEmbed(img_size*img_size, patch_size, 1, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model), num_layers=num_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.encoder(x)\n",
    "        return x.flatten(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, img_size=28, d_model=32, patch_size=7, nhead=2, num_layers=1, n_classes=10):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "\n",
    "        self.encoder = Encoder(img_size, d_model, patch_size, nhead, num_layers)\n",
    "        self.mlp = nn.Linear(d_model * num_patches, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = self.encoder(x)\n",
    "        return s, F.softmax(self.mlp(s), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13194"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Model(28, 32, 7, 2, 1, 10)\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=4) for ts in trainsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BEFORE = './mnist_transformer_before.pth'\n",
    "PATH_AFTER = './mnist_transformer_after.pth'\n",
    "# PATH_WITHOUT = './mnist_transformer_without.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training digit 0\n",
      "training digit 1\n",
      "training digit 2\n",
      "training digit 3\n",
      "training digit 4\n",
      "training digit 5\n",
      "training digit 6\n",
      "training digit 7\n",
      "training digit 8\n",
      "[1,  1001] loss: 4.538\n",
      "training digit 9\n",
      "training digit 0\n",
      "training digit 1\n",
      "training digit 2\n",
      "training digit 3\n",
      "training digit 4\n",
      "training digit 5\n",
      "training digit 6\n",
      "training digit 7\n",
      "training digit 8\n",
      "[2,  1001] loss: 4.538\n",
      "training digit 9\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    d = 0\n",
    "    for trainloader in trainloaders:\n",
    "        print(f'training digit {d}')\n",
    "        d += 1\n",
    "        for j, data in enumerate(trainloader):\n",
    "            x = data[0].view(batch_size, 1, 28, 28).float().to(device)\n",
    "            x2 = data[1].view(batch_size, 1, 28, 28).float().to(device)\n",
    "            label = data[2].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            s, out = net(x)\n",
    "            s2, out2 = net(x2)\n",
    "\n",
    "            # penalize far away embeddings\n",
    "            repr_loss = F.mse_loss(x, x2)\n",
    "\n",
    "            # Maintain variance above a threshold to prevent feature collapse\n",
    "            var_threshold = 0.5\n",
    "            std1 = torch.sqrt(s.var(dim=0) + 0.0001)\n",
    "            std2 = torch.sqrt(s2.var(dim=0) + 0.0001)\n",
    "            std_loss = torch.mean(F.relu(1 - std1)) / 2 + torch.mean(F.relu(1 - std2)) / 2\n",
    "            \n",
    "            # reduce co-variance such that feature detectors tend to learn unique things\n",
    "            # cov_loss = torch.mean((s1 - s1.mean(0)) * (s2 - s2.mean(0))) \n",
    "            s = s - s.mean(dim=0)\n",
    "            s2 = s2 - s2.mean(dim=0)\n",
    "            cov_s1 = (s.T @ s) / (batch_size - 1)\n",
    "            cov_s2 = (s2.T @ s2) / (batch_size - 1)\n",
    "            cov_loss = off_diagonal(cov_s1).pow_(2).sum().div(\n",
    "                28*28\n",
    "            ) + off_diagonal(cov_s2).pow_(2).sum().div(28*28)\n",
    "\n",
    "            \n",
    "            loss = torch.log(repr_loss + std_loss + cov_loss)\n",
    "            \n",
    "            # propagate back the gradients and update the model parameters/weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), PATH_BEFORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST(root='data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   101] loss: 0.114\n",
      "[1,   201] loss: 0.218\n",
      "[1,   301] loss: 0.312\n",
      "[1,   401] loss: 0.401\n",
      "[1,   501] loss: 0.488\n",
      "[1,   601] loss: 0.579\n",
      "[1,   701] loss: 0.669\n",
      "[1,   801] loss: 0.758\n",
      "[1,   901] loss: 0.844\n",
      "[1,  1001] loss: 0.930\n",
      "[1,  1101] loss: 1.018\n",
      "[1,  1201] loss: 1.102\n",
      "[1,  1301] loss: 1.189\n",
      "[1,  1401] loss: 1.275\n",
      "[1,  1501] loss: 1.359\n",
      "[1,  1601] loss: 1.441\n",
      "[1,  1701] loss: 1.522\n",
      "[1,  1801] loss: 1.601\n",
      "[1,  1901] loss: 1.682\n",
      "[1,  2001] loss: 1.766\n",
      "[1,  2101] loss: 1.850\n",
      "[1,  2201] loss: 1.929\n",
      "[1,  2301] loss: 2.012\n",
      "[1,  2401] loss: 2.091\n",
      "[1,  2501] loss: 2.173\n",
      "[1,  2601] loss: 2.252\n",
      "[1,  2701] loss: 2.333\n",
      "[1,  2801] loss: 2.415\n",
      "[1,  2901] loss: 2.494\n",
      "[1,  3001] loss: 2.576\n",
      "[1,  3101] loss: 2.657\n",
      "[1,  3201] loss: 2.738\n",
      "[1,  3301] loss: 2.818\n",
      "[1,  3401] loss: 2.898\n",
      "[1,  3501] loss: 2.975\n",
      "[1,  3601] loss: 3.056\n",
      "[1,  3701] loss: 3.136\n",
      "[1,  3801] loss: 3.214\n",
      "[1,  3901] loss: 3.291\n",
      "[1,  4001] loss: 3.370\n",
      "[1,  4101] loss: 3.450\n",
      "[1,  4201] loss: 3.530\n",
      "[1,  4301] loss: 3.610\n",
      "[1,  4401] loss: 3.692\n",
      "[1,  4501] loss: 3.771\n",
      "[1,  4601] loss: 3.849\n",
      "[1,  4701] loss: 3.930\n",
      "[1,  4801] loss: 4.008\n",
      "[1,  4901] loss: 4.090\n",
      "[1,  5001] loss: 4.169\n",
      "[1,  5101] loss: 4.246\n",
      "[1,  5201] loss: 4.325\n",
      "[1,  5301] loss: 4.402\n",
      "[1,  5401] loss: 4.482\n",
      "[1,  5501] loss: 4.560\n",
      "[1,  5601] loss: 4.638\n",
      "[1,  5701] loss: 4.717\n",
      "[1,  5801] loss: 4.796\n",
      "[1,  5901] loss: 4.876\n",
      "[1,  6001] loss: 4.952\n",
      "[1,  6101] loss: 5.030\n",
      "[1,  6201] loss: 5.107\n",
      "[1,  6301] loss: 5.189\n",
      "[1,  6401] loss: 5.267\n",
      "[1,  6501] loss: 5.347\n",
      "[1,  6601] loss: 5.423\n",
      "[1,  6701] loss: 5.502\n",
      "[1,  6801] loss: 5.582\n",
      "[1,  6901] loss: 5.665\n",
      "[1,  7001] loss: 5.745\n",
      "[1,  7101] loss: 5.826\n",
      "[1,  7201] loss: 5.905\n",
      "[1,  7301] loss: 5.990\n",
      "[1,  7401] loss: 6.066\n",
      "[1,  7501] loss: 6.146\n",
      "[1,  7601] loss: 6.224\n",
      "[1,  7701] loss: 6.304\n",
      "[1,  7801] loss: 6.383\n",
      "[1,  7901] loss: 6.462\n",
      "[1,  8001] loss: 6.542\n",
      "[1,  8101] loss: 6.622\n",
      "[1,  8201] loss: 6.703\n",
      "[1,  8301] loss: 6.783\n",
      "[1,  8401] loss: 6.862\n",
      "[1,  8501] loss: 6.944\n",
      "[1,  8601] loss: 7.021\n",
      "[1,  8701] loss: 7.103\n",
      "[1,  8801] loss: 7.186\n",
      "[1,  8901] loss: 7.270\n",
      "[1,  9001] loss: 7.350\n",
      "[1,  9101] loss: 7.428\n",
      "[1,  9201] loss: 7.505\n",
      "[1,  9301] loss: 7.584\n",
      "[1,  9401] loss: 7.664\n",
      "[1,  9501] loss: 7.743\n",
      "[1,  9601] loss: 7.821\n",
      "[1,  9701] loss: 7.898\n",
      "[1,  9801] loss: 7.976\n",
      "[1,  9901] loss: 8.054\n",
      "[1, 10001] loss: 8.130\n",
      "[1, 10101] loss: 8.209\n",
      "[1, 10201] loss: 8.288\n",
      "[1, 10301] loss: 8.372\n",
      "[1, 10401] loss: 8.449\n",
      "[1, 10501] loss: 8.526\n",
      "[1, 10601] loss: 8.603\n",
      "[1, 10701] loss: 8.680\n",
      "[1, 10801] loss: 8.759\n",
      "[1, 10901] loss: 8.835\n",
      "[1, 11001] loss: 8.914\n",
      "[1, 11101] loss: 8.990\n",
      "[1, 11201] loss: 9.067\n",
      "[1, 11301] loss: 9.147\n",
      "[1, 11401] loss: 9.223\n",
      "[1, 11501] loss: 9.301\n",
      "[1, 11601] loss: 9.380\n",
      "[1, 11701] loss: 9.463\n",
      "[1, 11801] loss: 9.545\n",
      "[1, 11901] loss: 9.623\n",
      "[1, 12001] loss: 9.700\n",
      "[1, 12101] loss: 9.779\n",
      "[1, 12201] loss: 9.855\n",
      "[1, 12301] loss: 9.938\n",
      "[1, 12401] loss: 10.018\n",
      "[1, 12501] loss: 10.099\n",
      "[1, 12601] loss: 10.180\n",
      "[1, 12701] loss: 10.262\n",
      "[1, 12801] loss: 10.340\n",
      "[1, 12901] loss: 10.420\n",
      "[1, 13001] loss: 10.502\n",
      "[1, 13101] loss: 10.587\n",
      "[1, 13201] loss: 10.668\n",
      "[1, 13301] loss: 10.744\n",
      "[1, 13401] loss: 10.823\n",
      "[1, 13501] loss: 10.901\n",
      "[1, 13601] loss: 10.978\n",
      "[1, 13701] loss: 11.058\n",
      "[1, 13801] loss: 11.137\n",
      "[1, 13901] loss: 11.215\n",
      "[1, 14001] loss: 11.296\n",
      "[1, 14101] loss: 11.375\n",
      "[1, 14201] loss: 11.455\n",
      "[1, 14301] loss: 11.537\n",
      "[1, 14401] loss: 11.618\n",
      "[1, 14501] loss: 11.697\n",
      "[1, 14601] loss: 11.775\n",
      "[1, 14701] loss: 11.858\n",
      "[1, 14801] loss: 11.941\n",
      "[1, 14901] loss: 12.020\n",
      "[1, 15001] loss: 12.098\n",
      "[1, 15101] loss: 12.173\n",
      "[1, 15201] loss: 12.252\n",
      "[1, 15301] loss: 12.329\n",
      "[1, 15401] loss: 12.408\n",
      "[1, 15501] loss: 12.485\n",
      "[1, 15601] loss: 12.562\n",
      "[1, 15701] loss: 12.638\n",
      "[1, 15801] loss: 12.722\n",
      "[1, 15901] loss: 12.805\n",
      "[1, 16001] loss: 12.884\n",
      "[1, 16101] loss: 12.964\n",
      "[1, 16201] loss: 13.044\n",
      "[1, 16301] loss: 13.120\n",
      "[1, 16401] loss: 13.196\n",
      "[1, 16501] loss: 13.272\n",
      "[1, 16601] loss: 13.349\n",
      "[1, 16701] loss: 13.428\n",
      "[1, 16801] loss: 13.507\n",
      "[1, 16901] loss: 13.582\n",
      "[1, 17001] loss: 13.662\n",
      "[1, 17101] loss: 13.743\n",
      "[1, 17201] loss: 13.823\n",
      "[1, 17301] loss: 13.903\n",
      "[1, 17401] loss: 13.978\n",
      "[1, 17501] loss: 14.059\n",
      "[1, 17601] loss: 14.139\n",
      "[1, 17701] loss: 14.221\n",
      "[1, 17801] loss: 14.301\n",
      "[1, 17901] loss: 14.381\n",
      "[1, 18001] loss: 14.459\n",
      "[1, 18101] loss: 14.536\n",
      "[1, 18201] loss: 14.611\n",
      "[1, 18301] loss: 14.687\n",
      "[1, 18401] loss: 14.763\n",
      "[1, 18501] loss: 14.843\n",
      "[1, 18601] loss: 14.920\n",
      "[1, 18701] loss: 14.998\n",
      "[1, 18801] loss: 15.076\n",
      "[1, 18901] loss: 15.154\n",
      "[1, 19001] loss: 15.231\n",
      "[1, 19101] loss: 15.308\n",
      "[1, 19201] loss: 15.385\n",
      "[1, 19301] loss: 15.463\n",
      "[1, 19401] loss: 15.543\n",
      "[1, 19501] loss: 15.621\n",
      "[1, 19601] loss: 15.699\n",
      "[1, 19701] loss: 15.774\n",
      "[1, 19801] loss: 15.850\n",
      "[1, 19901] loss: 15.929\n",
      "[1, 20001] loss: 16.008\n",
      "[1, 20101] loss: 16.087\n",
      "[1, 20201] loss: 16.163\n",
      "[1, 20301] loss: 16.240\n",
      "[1, 20401] loss: 16.316\n",
      "[1, 20501] loss: 16.392\n",
      "[1, 20601] loss: 16.469\n",
      "[1, 20701] loss: 16.546\n",
      "[1, 20801] loss: 16.626\n",
      "[1, 20901] loss: 16.703\n",
      "[1, 21001] loss: 16.783\n",
      "[1, 21101] loss: 16.859\n",
      "[1, 21201] loss: 16.936\n",
      "[1, 21301] loss: 17.011\n",
      "[1, 21401] loss: 17.087\n",
      "[1, 21501] loss: 17.165\n",
      "[1, 21601] loss: 17.241\n",
      "[1, 21701] loss: 17.316\n",
      "[1, 21801] loss: 17.392\n",
      "[1, 21901] loss: 17.469\n",
      "[1, 22001] loss: 17.546\n",
      "[1, 22101] loss: 17.623\n",
      "[1, 22201] loss: 17.702\n",
      "[1, 22301] loss: 17.782\n",
      "[1, 22401] loss: 17.860\n",
      "[1, 22501] loss: 17.942\n",
      "[1, 22601] loss: 18.026\n",
      "[1, 22701] loss: 18.106\n",
      "[1, 22801] loss: 18.185\n",
      "[1, 22901] loss: 18.265\n",
      "[1, 23001] loss: 18.341\n",
      "[1, 23101] loss: 18.419\n",
      "[1, 23201] loss: 18.498\n",
      "[1, 23301] loss: 18.575\n",
      "[1, 23401] loss: 18.653\n",
      "[1, 23501] loss: 18.734\n",
      "[1, 23601] loss: 18.811\n",
      "[1, 23701] loss: 18.890\n",
      "[1, 23801] loss: 18.969\n",
      "[1, 23901] loss: 19.047\n",
      "[1, 24001] loss: 19.123\n",
      "[1, 24101] loss: 19.201\n",
      "[1, 24201] loss: 19.279\n",
      "[1, 24301] loss: 19.358\n",
      "[1, 24401] loss: 19.435\n",
      "[1, 24501] loss: 19.513\n",
      "[1, 24601] loss: 19.591\n",
      "[1, 24701] loss: 19.670\n",
      "[1, 24801] loss: 19.749\n",
      "[1, 24901] loss: 19.827\n",
      "[1, 25001] loss: 19.909\n",
      "[1, 25101] loss: 19.986\n",
      "[1, 25201] loss: 20.064\n",
      "[1, 25301] loss: 20.142\n",
      "[1, 25401] loss: 20.221\n",
      "[1, 25501] loss: 20.297\n",
      "[1, 25601] loss: 20.373\n",
      "[1, 25701] loss: 20.450\n",
      "[1, 25801] loss: 20.528\n",
      "[1, 25901] loss: 20.605\n",
      "[1, 26001] loss: 20.684\n",
      "[1, 26101] loss: 20.761\n",
      "[1, 26201] loss: 20.839\n",
      "[1, 26301] loss: 20.915\n",
      "[1, 26401] loss: 20.994\n",
      "[1, 26501] loss: 21.072\n",
      "[1, 26601] loss: 21.150\n",
      "[1, 26701] loss: 21.230\n",
      "[1, 26801] loss: 21.312\n",
      "[1, 26901] loss: 21.390\n",
      "[1, 27001] loss: 21.468\n",
      "[1, 27101] loss: 21.546\n",
      "[1, 27201] loss: 21.628\n",
      "[1, 27301] loss: 21.707\n",
      "[1, 27401] loss: 21.784\n",
      "[1, 27501] loss: 21.862\n",
      "[1, 27601] loss: 21.939\n",
      "[1, 27701] loss: 22.018\n",
      "[1, 27801] loss: 22.095\n",
      "[1, 27901] loss: 22.171\n",
      "[1, 28001] loss: 22.249\n",
      "[1, 28101] loss: 22.324\n",
      "[1, 28201] loss: 22.402\n",
      "[1, 28301] loss: 22.479\n",
      "[1, 28401] loss: 22.558\n",
      "[1, 28501] loss: 22.633\n",
      "[1, 28601] loss: 22.710\n",
      "[1, 28701] loss: 22.790\n",
      "[1, 28801] loss: 22.868\n",
      "[1, 28901] loss: 22.946\n",
      "[1, 29001] loss: 23.023\n",
      "[1, 29101] loss: 23.100\n",
      "[1, 29201] loss: 23.179\n",
      "[1, 29301] loss: 23.257\n",
      "[1, 29401] loss: 23.336\n",
      "[1, 29501] loss: 23.413\n",
      "[1, 29601] loss: 23.488\n",
      "[1, 29701] loss: 23.566\n",
      "[1, 29801] loss: 23.645\n",
      "[1, 29901] loss: 23.729\n",
      "[1, 30001] loss: 23.811\n",
      "[1, 30101] loss: 23.890\n",
      "[1, 30201] loss: 23.970\n",
      "[1, 30301] loss: 24.047\n",
      "[1, 30401] loss: 24.126\n",
      "[1, 30501] loss: 24.203\n",
      "[1, 30601] loss: 24.285\n",
      "[1, 30701] loss: 24.362\n",
      "[1, 30801] loss: 24.440\n",
      "[1, 30901] loss: 24.518\n",
      "[1, 31001] loss: 24.596\n",
      "[1, 31101] loss: 24.673\n",
      "[1, 31201] loss: 24.751\n",
      "[1, 31301] loss: 24.830\n",
      "[1, 31401] loss: 24.911\n",
      "[1, 31501] loss: 24.989\n",
      "[1, 31601] loss: 25.067\n",
      "[1, 31701] loss: 25.147\n",
      "[1, 31801] loss: 25.229\n",
      "[1, 31901] loss: 25.309\n",
      "[1, 32001] loss: 25.386\n",
      "[1, 32101] loss: 25.465\n",
      "[1, 32201] loss: 25.544\n",
      "[1, 32301] loss: 25.623\n",
      "[1, 32401] loss: 25.702\n",
      "[1, 32501] loss: 25.782\n",
      "[1, 32601] loss: 25.859\n",
      "[1, 32701] loss: 25.937\n",
      "[1, 32801] loss: 26.014\n",
      "[1, 32901] loss: 26.093\n",
      "[1, 33001] loss: 26.168\n",
      "[1, 33101] loss: 26.243\n",
      "[1, 33201] loss: 26.320\n",
      "[1, 33301] loss: 26.395\n",
      "[1, 33401] loss: 26.473\n",
      "[1, 33501] loss: 26.550\n",
      "[1, 33601] loss: 26.627\n",
      "[1, 33701] loss: 26.702\n",
      "[1, 33801] loss: 26.780\n",
      "[1, 33901] loss: 26.856\n",
      "[1, 34001] loss: 26.931\n",
      "[1, 34101] loss: 27.008\n",
      "[1, 34201] loss: 27.083\n",
      "[1, 34301] loss: 27.157\n",
      "[1, 34401] loss: 27.235\n",
      "[1, 34501] loss: 27.312\n",
      "[1, 34601] loss: 27.391\n",
      "[1, 34701] loss: 27.470\n",
      "[1, 34801] loss: 27.548\n",
      "[1, 34901] loss: 27.627\n",
      "[1, 35001] loss: 27.704\n",
      "[1, 35101] loss: 27.782\n",
      "[1, 35201] loss: 27.859\n",
      "[1, 35301] loss: 27.937\n",
      "[1, 35401] loss: 28.013\n",
      "[1, 35501] loss: 28.092\n",
      "[1, 35601] loss: 28.167\n",
      "[1, 35701] loss: 28.247\n",
      "[1, 35801] loss: 28.325\n",
      "[1, 35901] loss: 28.401\n",
      "[1, 36001] loss: 28.479\n",
      "[1, 36101] loss: 28.559\n",
      "[1, 36201] loss: 28.636\n",
      "[1, 36301] loss: 28.713\n",
      "[1, 36401] loss: 28.789\n",
      "[1, 36501] loss: 28.866\n",
      "[1, 36601] loss: 28.945\n",
      "[1, 36701] loss: 29.020\n",
      "[1, 36801] loss: 29.097\n",
      "[1, 36901] loss: 29.176\n",
      "[1, 37001] loss: 29.255\n",
      "[1, 37101] loss: 29.334\n",
      "[1, 37201] loss: 29.411\n",
      "[1, 37301] loss: 29.490\n",
      "[1, 37401] loss: 29.572\n",
      "[1, 37501] loss: 29.651\n",
      "[1, 37601] loss: 29.729\n",
      "[1, 37701] loss: 29.806\n",
      "[1, 37801] loss: 29.885\n",
      "[1, 37901] loss: 29.964\n",
      "[1, 38001] loss: 30.043\n",
      "[1, 38101] loss: 30.121\n",
      "[1, 38201] loss: 30.198\n",
      "[1, 38301] loss: 30.276\n",
      "[1, 38401] loss: 30.356\n",
      "[1, 38501] loss: 30.432\n",
      "[1, 38601] loss: 30.510\n",
      "[1, 38701] loss: 30.592\n",
      "[1, 38801] loss: 30.669\n",
      "[1, 38901] loss: 30.745\n",
      "[1, 39001] loss: 30.821\n",
      "[1, 39101] loss: 30.896\n",
      "[1, 39201] loss: 30.975\n",
      "[1, 39301] loss: 31.053\n",
      "[1, 39401] loss: 31.137\n",
      "[1, 39501] loss: 31.217\n",
      "[1, 39601] loss: 31.294\n",
      "[1, 39701] loss: 31.374\n",
      "[1, 39801] loss: 31.452\n",
      "[1, 39901] loss: 31.532\n",
      "[1, 40001] loss: 31.608\n",
      "[1, 40101] loss: 31.685\n",
      "[1, 40201] loss: 31.761\n",
      "[1, 40301] loss: 31.839\n",
      "[1, 40401] loss: 31.917\n",
      "[1, 40501] loss: 31.992\n",
      "[1, 40601] loss: 32.069\n",
      "[1, 40701] loss: 32.146\n",
      "[1, 40801] loss: 32.223\n",
      "[1, 40901] loss: 32.299\n",
      "[1, 41001] loss: 32.375\n",
      "[1, 41101] loss: 32.454\n",
      "[1, 41201] loss: 32.531\n",
      "[1, 41301] loss: 32.608\n",
      "[1, 41401] loss: 32.685\n",
      "[1, 41501] loss: 32.764\n",
      "[1, 41601] loss: 32.842\n",
      "[1, 41701] loss: 32.918\n",
      "[1, 41801] loss: 32.995\n",
      "[1, 41901] loss: 33.072\n",
      "[1, 42001] loss: 33.148\n",
      "[1, 42101] loss: 33.224\n",
      "[1, 42201] loss: 33.304\n",
      "[1, 42301] loss: 33.382\n",
      "[1, 42401] loss: 33.464\n",
      "[1, 42501] loss: 33.544\n",
      "[1, 42601] loss: 33.623\n",
      "[1, 42701] loss: 33.700\n",
      "[1, 42801] loss: 33.777\n",
      "[1, 42901] loss: 33.856\n",
      "[1, 43001] loss: 33.937\n",
      "[1, 43101] loss: 34.018\n",
      "[1, 43201] loss: 34.097\n",
      "[1, 43301] loss: 34.176\n",
      "[1, 43401] loss: 34.250\n",
      "[1, 43501] loss: 34.326\n",
      "[1, 43601] loss: 34.403\n",
      "[1, 43701] loss: 34.482\n",
      "[1, 43801] loss: 34.560\n",
      "[1, 43901] loss: 34.638\n",
      "[1, 44001] loss: 34.715\n",
      "[1, 44101] loss: 34.790\n",
      "[1, 44201] loss: 34.870\n",
      "[1, 44301] loss: 34.949\n",
      "[1, 44401] loss: 35.026\n",
      "[1, 44501] loss: 35.103\n",
      "[1, 44601] loss: 35.179\n",
      "[1, 44701] loss: 35.254\n",
      "[1, 44801] loss: 35.330\n",
      "[1, 44901] loss: 35.408\n",
      "[1, 45001] loss: 35.487\n",
      "[1, 45101] loss: 35.567\n",
      "[1, 45201] loss: 35.643\n",
      "[1, 45301] loss: 35.719\n",
      "[1, 45401] loss: 35.797\n",
      "[1, 45501] loss: 35.877\n",
      "[1, 45601] loss: 35.956\n",
      "[1, 45701] loss: 36.033\n",
      "[1, 45801] loss: 36.112\n",
      "[1, 45901] loss: 36.190\n",
      "[1, 46001] loss: 36.270\n",
      "[1, 46101] loss: 36.349\n",
      "[1, 46201] loss: 36.427\n",
      "[1, 46301] loss: 36.507\n",
      "[1, 46401] loss: 36.589\n",
      "[1, 46501] loss: 36.669\n",
      "[1, 46601] loss: 36.744\n",
      "[1, 46701] loss: 36.821\n",
      "[1, 46801] loss: 36.899\n",
      "[1, 46901] loss: 36.978\n",
      "[1, 47001] loss: 37.054\n",
      "[1, 47101] loss: 37.133\n",
      "[1, 47201] loss: 37.208\n",
      "[1, 47301] loss: 37.291\n",
      "[1, 47401] loss: 37.368\n",
      "[1, 47501] loss: 37.444\n",
      "[1, 47601] loss: 37.523\n",
      "[1, 47701] loss: 37.602\n",
      "[1, 47801] loss: 37.678\n",
      "[1, 47901] loss: 37.754\n",
      "[1, 48001] loss: 37.834\n",
      "[1, 48101] loss: 37.910\n",
      "[1, 48201] loss: 37.987\n",
      "[1, 48301] loss: 38.062\n",
      "[1, 48401] loss: 38.140\n",
      "[1, 48501] loss: 38.216\n",
      "[1, 48601] loss: 38.294\n",
      "[1, 48701] loss: 38.370\n",
      "[1, 48801] loss: 38.445\n",
      "[1, 48901] loss: 38.521\n",
      "[1, 49001] loss: 38.602\n",
      "[1, 49101] loss: 38.682\n",
      "[1, 49201] loss: 38.763\n",
      "[1, 49301] loss: 38.843\n",
      "[1, 49401] loss: 38.919\n",
      "[1, 49501] loss: 38.998\n",
      "[1, 49601] loss: 39.079\n",
      "[1, 49701] loss: 39.157\n",
      "[1, 49801] loss: 39.233\n",
      "[1, 49901] loss: 39.311\n",
      "[1, 50001] loss: 39.388\n",
      "[1, 50101] loss: 39.467\n",
      "[1, 50201] loss: 39.544\n",
      "[1, 50301] loss: 39.622\n",
      "[1, 50401] loss: 39.700\n",
      "[1, 50501] loss: 39.780\n",
      "[1, 50601] loss: 39.856\n",
      "[1, 50701] loss: 39.934\n",
      "[1, 50801] loss: 40.010\n",
      "[1, 50901] loss: 40.086\n",
      "[1, 51001] loss: 40.161\n",
      "[1, 51101] loss: 40.236\n",
      "[1, 51201] loss: 40.314\n",
      "[1, 51301] loss: 40.393\n",
      "[1, 51401] loss: 40.471\n",
      "[1, 51501] loss: 40.546\n",
      "[1, 51601] loss: 40.622\n",
      "[1, 51701] loss: 40.701\n",
      "[1, 51801] loss: 40.776\n",
      "[1, 51901] loss: 40.853\n",
      "[1, 52001] loss: 40.930\n",
      "[1, 52101] loss: 41.009\n",
      "[1, 52201] loss: 41.090\n",
      "[1, 52301] loss: 41.170\n",
      "[1, 52401] loss: 41.248\n",
      "[1, 52501] loss: 41.322\n",
      "[1, 52601] loss: 41.398\n",
      "[1, 52701] loss: 41.474\n",
      "[1, 52801] loss: 41.553\n",
      "[1, 52901] loss: 41.633\n",
      "[1, 53001] loss: 41.714\n",
      "[1, 53101] loss: 41.792\n",
      "[1, 53201] loss: 41.870\n",
      "[1, 53301] loss: 41.947\n",
      "[1, 53401] loss: 42.025\n",
      "[1, 53501] loss: 42.100\n",
      "[1, 53601] loss: 42.177\n",
      "[1, 53701] loss: 42.255\n",
      "[1, 53801] loss: 42.332\n",
      "[1, 53901] loss: 42.408\n",
      "[1, 54001] loss: 42.489\n",
      "[1, 54101] loss: 42.568\n",
      "[1, 54201] loss: 42.644\n",
      "[1, 54301] loss: 42.718\n",
      "[1, 54401] loss: 42.793\n",
      "[1, 54501] loss: 42.869\n",
      "[1, 54601] loss: 42.949\n",
      "[1, 54701] loss: 43.024\n",
      "[1, 54801] loss: 43.102\n",
      "[1, 54901] loss: 43.180\n",
      "[1, 55001] loss: 43.259\n",
      "[1, 55101] loss: 43.336\n",
      "[1, 55201] loss: 43.414\n",
      "[1, 55301] loss: 43.489\n",
      "[1, 55401] loss: 43.566\n",
      "[1, 55501] loss: 43.643\n",
      "[1, 55601] loss: 43.719\n",
      "[1, 55701] loss: 43.797\n",
      "[1, 55801] loss: 43.874\n",
      "[1, 55901] loss: 43.950\n",
      "[1, 56001] loss: 44.026\n",
      "[1, 56101] loss: 44.102\n",
      "[1, 56201] loss: 44.177\n",
      "[1, 56301] loss: 44.256\n",
      "[1, 56401] loss: 44.335\n",
      "[1, 56501] loss: 44.412\n",
      "[1, 56601] loss: 44.486\n",
      "[1, 56701] loss: 44.564\n",
      "[1, 56801] loss: 44.639\n",
      "[1, 56901] loss: 44.716\n",
      "[1, 57001] loss: 44.792\n",
      "[1, 57101] loss: 44.869\n",
      "[1, 57201] loss: 44.944\n",
      "[1, 57301] loss: 45.023\n",
      "[1, 57401] loss: 45.099\n",
      "[1, 57501] loss: 45.175\n",
      "[1, 57601] loss: 45.252\n",
      "[1, 57701] loss: 45.330\n",
      "[1, 57801] loss: 45.410\n",
      "[1, 57901] loss: 45.488\n",
      "[1, 58001] loss: 45.564\n",
      "[1, 58101] loss: 45.639\n",
      "[1, 58201] loss: 45.715\n",
      "[1, 58301] loss: 45.789\n",
      "[1, 58401] loss: 45.864\n",
      "[1, 58501] loss: 45.940\n",
      "[1, 58601] loss: 46.016\n",
      "[1, 58701] loss: 46.090\n",
      "[1, 58801] loss: 46.165\n",
      "[1, 58901] loss: 46.242\n",
      "[1, 59001] loss: 46.316\n",
      "[1, 59101] loss: 46.389\n",
      "[1, 59201] loss: 46.463\n",
      "[1, 59301] loss: 46.538\n",
      "[1, 59401] loss: 46.615\n",
      "[1, 59501] loss: 46.691\n",
      "[1, 59601] loss: 46.766\n",
      "[1, 59701] loss: 46.840\n",
      "[1, 59801] loss: 46.917\n",
      "[1, 59901] loss: 46.991\n",
      "[1, 60001] loss: 47.068\n",
      "[2,   101] loss: 0.076\n",
      "[2,   201] loss: 0.155\n",
      "[2,   301] loss: 0.233\n",
      "[2,   401] loss: 0.310\n",
      "[2,   501] loss: 0.386\n",
      "[2,   601] loss: 0.463\n",
      "[2,   701] loss: 0.538\n",
      "[2,   801] loss: 0.616\n",
      "[2,   901] loss: 0.696\n",
      "[2,  1001] loss: 0.774\n",
      "[2,  1101] loss: 0.851\n",
      "[2,  1201] loss: 0.930\n",
      "[2,  1301] loss: 1.008\n",
      "[2,  1401] loss: 1.086\n",
      "[2,  1501] loss: 1.162\n",
      "[2,  1601] loss: 1.237\n",
      "[2,  1701] loss: 1.313\n",
      "[2,  1801] loss: 1.389\n",
      "[2,  1901] loss: 1.464\n",
      "[2,  2001] loss: 1.541\n",
      "[2,  2101] loss: 1.619\n",
      "[2,  2201] loss: 1.693\n",
      "[2,  2301] loss: 1.768\n",
      "[2,  2401] loss: 1.845\n",
      "[2,  2501] loss: 1.925\n",
      "[2,  2601] loss: 2.000\n",
      "[2,  2701] loss: 2.077\n",
      "[2,  2801] loss: 2.154\n",
      "[2,  2901] loss: 2.229\n",
      "[2,  3001] loss: 2.306\n",
      "[2,  3101] loss: 2.384\n",
      "[2,  3201] loss: 2.459\n",
      "[2,  3301] loss: 2.537\n",
      "[2,  3401] loss: 2.614\n",
      "[2,  3501] loss: 2.690\n",
      "[2,  3601] loss: 2.766\n",
      "[2,  3701] loss: 2.844\n",
      "[2,  3801] loss: 2.922\n",
      "[2,  3901] loss: 2.997\n",
      "[2,  4001] loss: 3.073\n",
      "[2,  4101] loss: 3.151\n",
      "[2,  4201] loss: 3.229\n",
      "[2,  4301] loss: 3.304\n",
      "[2,  4401] loss: 3.381\n",
      "[2,  4501] loss: 3.457\n",
      "[2,  4601] loss: 3.533\n",
      "[2,  4701] loss: 3.610\n",
      "[2,  4801] loss: 3.687\n",
      "[2,  4901] loss: 3.766\n",
      "[2,  5001] loss: 3.844\n",
      "[2,  5101] loss: 3.922\n",
      "[2,  5201] loss: 3.999\n",
      "[2,  5301] loss: 4.074\n",
      "[2,  5401] loss: 4.151\n",
      "[2,  5501] loss: 4.226\n",
      "[2,  5601] loss: 4.303\n",
      "[2,  5701] loss: 4.380\n",
      "[2,  5801] loss: 4.458\n",
      "[2,  5901] loss: 4.537\n",
      "[2,  6001] loss: 4.613\n",
      "[2,  6101] loss: 4.686\n",
      "[2,  6201] loss: 4.762\n",
      "[2,  6301] loss: 4.839\n",
      "[2,  6401] loss: 4.916\n",
      "[2,  6501] loss: 4.993\n",
      "[2,  6601] loss: 5.067\n",
      "[2,  6701] loss: 5.144\n",
      "[2,  6801] loss: 5.221\n",
      "[2,  6901] loss: 5.298\n",
      "[2,  7001] loss: 5.377\n",
      "[2,  7101] loss: 5.454\n",
      "[2,  7201] loss: 5.532\n",
      "[2,  7301] loss: 5.612\n",
      "[2,  7401] loss: 5.689\n",
      "[2,  7501] loss: 5.763\n",
      "[2,  7601] loss: 5.839\n",
      "[2,  7701] loss: 5.915\n",
      "[2,  7801] loss: 5.994\n",
      "[2,  7901] loss: 6.074\n",
      "[2,  8001] loss: 6.152\n",
      "[2,  8101] loss: 6.230\n",
      "[2,  8201] loss: 6.306\n",
      "[2,  8301] loss: 6.385\n",
      "[2,  8401] loss: 6.461\n",
      "[2,  8501] loss: 6.539\n",
      "[2,  8601] loss: 6.616\n",
      "[2,  8701] loss: 6.695\n",
      "[2,  8801] loss: 6.779\n",
      "[2,  8901] loss: 6.858\n",
      "[2,  9001] loss: 6.935\n",
      "[2,  9101] loss: 7.011\n",
      "[2,  9201] loss: 7.088\n",
      "[2,  9301] loss: 7.165\n",
      "[2,  9401] loss: 7.244\n",
      "[2,  9501] loss: 7.320\n",
      "[2,  9601] loss: 7.395\n",
      "[2,  9701] loss: 7.471\n",
      "[2,  9801] loss: 7.549\n",
      "[2,  9901] loss: 7.626\n",
      "[2, 10001] loss: 7.701\n",
      "[2, 10101] loss: 7.778\n",
      "[2, 10201] loss: 7.855\n",
      "[2, 10301] loss: 7.941\n",
      "[2, 10401] loss: 8.017\n",
      "[2, 10501] loss: 8.092\n",
      "[2, 10601] loss: 8.166\n",
      "[2, 10701] loss: 8.240\n",
      "[2, 10801] loss: 8.320\n",
      "[2, 10901] loss: 8.396\n",
      "[2, 11001] loss: 8.474\n",
      "[2, 11101] loss: 8.550\n",
      "[2, 11201] loss: 8.625\n",
      "[2, 11301] loss: 8.704\n",
      "[2, 11401] loss: 8.780\n",
      "[2, 11501] loss: 8.856\n",
      "[2, 11601] loss: 8.936\n",
      "[2, 11701] loss: 9.015\n",
      "[2, 11801] loss: 9.095\n",
      "[2, 11901] loss: 9.172\n",
      "[2, 12001] loss: 9.251\n",
      "[2, 12101] loss: 9.330\n",
      "[2, 12201] loss: 9.406\n",
      "[2, 12301] loss: 9.486\n",
      "[2, 12401] loss: 9.565\n",
      "[2, 12501] loss: 9.643\n",
      "[2, 12601] loss: 9.720\n",
      "[2, 12701] loss: 9.799\n",
      "[2, 12801] loss: 9.875\n",
      "[2, 12901] loss: 9.951\n",
      "[2, 13001] loss: 10.032\n",
      "[2, 13101] loss: 10.114\n",
      "[2, 13201] loss: 10.194\n",
      "[2, 13301] loss: 10.269\n",
      "[2, 13401] loss: 10.348\n",
      "[2, 13501] loss: 10.423\n",
      "[2, 13601] loss: 10.499\n",
      "[2, 13701] loss: 10.578\n",
      "[2, 13801] loss: 10.654\n",
      "[2, 13901] loss: 10.730\n",
      "[2, 14001] loss: 10.809\n",
      "[2, 14101] loss: 10.887\n",
      "[2, 14201] loss: 10.964\n",
      "[2, 14301] loss: 11.040\n",
      "[2, 14401] loss: 11.118\n",
      "[2, 14501] loss: 11.195\n",
      "[2, 14601] loss: 11.272\n",
      "[2, 14701] loss: 11.356\n",
      "[2, 14801] loss: 11.434\n",
      "[2, 14901] loss: 11.513\n",
      "[2, 15001] loss: 11.591\n",
      "[2, 15101] loss: 11.667\n",
      "[2, 15201] loss: 11.746\n",
      "[2, 15301] loss: 11.822\n",
      "[2, 15401] loss: 11.900\n",
      "[2, 15501] loss: 11.977\n",
      "[2, 15601] loss: 12.055\n",
      "[2, 15701] loss: 12.133\n",
      "[2, 15801] loss: 12.214\n",
      "[2, 15901] loss: 12.295\n",
      "[2, 16001] loss: 12.373\n",
      "[2, 16101] loss: 12.452\n",
      "[2, 16201] loss: 12.528\n",
      "[2, 16301] loss: 12.605\n",
      "[2, 16401] loss: 12.681\n",
      "[2, 16501] loss: 12.756\n",
      "[2, 16601] loss: 12.832\n",
      "[2, 16701] loss: 12.910\n",
      "[2, 16801] loss: 12.988\n",
      "[2, 16901] loss: 13.064\n",
      "[2, 17001] loss: 13.141\n",
      "[2, 17101] loss: 13.219\n",
      "[2, 17201] loss: 13.299\n",
      "[2, 17301] loss: 13.377\n",
      "[2, 17401] loss: 13.452\n",
      "[2, 17501] loss: 13.529\n",
      "[2, 17601] loss: 13.607\n",
      "[2, 17701] loss: 13.683\n",
      "[2, 17801] loss: 13.762\n",
      "[2, 17901] loss: 13.841\n",
      "[2, 18001] loss: 13.918\n",
      "[2, 18101] loss: 13.996\n",
      "[2, 18201] loss: 14.072\n",
      "[2, 18301] loss: 14.147\n",
      "[2, 18401] loss: 14.223\n",
      "[2, 18501] loss: 14.304\n",
      "[2, 18601] loss: 14.380\n",
      "[2, 18701] loss: 14.459\n",
      "[2, 18801] loss: 14.535\n",
      "[2, 18901] loss: 14.610\n",
      "[2, 19001] loss: 14.685\n",
      "[2, 19101] loss: 14.763\n",
      "[2, 19201] loss: 14.840\n",
      "[2, 19301] loss: 14.918\n",
      "[2, 19401] loss: 14.995\n",
      "[2, 19501] loss: 15.073\n",
      "[2, 19601] loss: 15.151\n",
      "[2, 19701] loss: 15.227\n",
      "[2, 19801] loss: 15.302\n",
      "[2, 19901] loss: 15.379\n",
      "[2, 20001] loss: 15.456\n",
      "[2, 20101] loss: 15.536\n",
      "[2, 20201] loss: 15.613\n",
      "[2, 20301] loss: 15.692\n",
      "[2, 20401] loss: 15.767\n",
      "[2, 20501] loss: 15.843\n",
      "[2, 20601] loss: 15.919\n",
      "[2, 20701] loss: 15.996\n",
      "[2, 20801] loss: 16.074\n",
      "[2, 20901] loss: 16.151\n",
      "[2, 21001] loss: 16.228\n",
      "[2, 21101] loss: 16.306\n",
      "[2, 21201] loss: 16.382\n",
      "[2, 21301] loss: 16.456\n",
      "[2, 21401] loss: 16.532\n",
      "[2, 21501] loss: 16.609\n",
      "[2, 21601] loss: 16.684\n",
      "[2, 21701] loss: 16.762\n",
      "[2, 21801] loss: 16.837\n",
      "[2, 21901] loss: 16.913\n",
      "[2, 22001] loss: 16.991\n",
      "[2, 22101] loss: 17.067\n",
      "[2, 22201] loss: 17.145\n",
      "[2, 22301] loss: 17.224\n",
      "[2, 22401] loss: 17.298\n",
      "[2, 22501] loss: 17.376\n",
      "[2, 22601] loss: 17.456\n",
      "[2, 22701] loss: 17.536\n",
      "[2, 22801] loss: 17.616\n",
      "[2, 22901] loss: 17.696\n",
      "[2, 23001] loss: 17.771\n",
      "[2, 23101] loss: 17.849\n",
      "[2, 23201] loss: 17.925\n",
      "[2, 23301] loss: 18.001\n",
      "[2, 23401] loss: 18.078\n",
      "[2, 23501] loss: 18.155\n",
      "[2, 23601] loss: 18.232\n",
      "[2, 23701] loss: 18.308\n",
      "[2, 23801] loss: 18.387\n",
      "[2, 23901] loss: 18.466\n",
      "[2, 24001] loss: 18.543\n",
      "[2, 24101] loss: 18.619\n",
      "[2, 24201] loss: 18.695\n",
      "[2, 24301] loss: 18.773\n",
      "[2, 24401] loss: 18.847\n",
      "[2, 24501] loss: 18.925\n",
      "[2, 24601] loss: 19.002\n",
      "[2, 24701] loss: 19.081\n",
      "[2, 24801] loss: 19.159\n",
      "[2, 24901] loss: 19.234\n",
      "[2, 25001] loss: 19.313\n",
      "[2, 25101] loss: 19.388\n",
      "[2, 25201] loss: 19.464\n",
      "[2, 25301] loss: 19.542\n",
      "[2, 25401] loss: 19.618\n",
      "[2, 25501] loss: 19.693\n",
      "[2, 25601] loss: 19.769\n",
      "[2, 25701] loss: 19.845\n",
      "[2, 25801] loss: 19.922\n",
      "[2, 25901] loss: 20.000\n",
      "[2, 26001] loss: 20.078\n",
      "[2, 26101] loss: 20.156\n",
      "[2, 26201] loss: 20.231\n",
      "[2, 26301] loss: 20.308\n",
      "[2, 26401] loss: 20.386\n",
      "[2, 26501] loss: 20.463\n",
      "[2, 26601] loss: 20.541\n",
      "[2, 26701] loss: 20.620\n",
      "[2, 26801] loss: 20.700\n",
      "[2, 26901] loss: 20.779\n",
      "[2, 27001] loss: 20.855\n",
      "[2, 27101] loss: 20.930\n",
      "[2, 27201] loss: 21.009\n",
      "[2, 27301] loss: 21.085\n",
      "[2, 27401] loss: 21.161\n",
      "[2, 27501] loss: 21.236\n",
      "[2, 27601] loss: 21.313\n",
      "[2, 27701] loss: 21.389\n",
      "[2, 27801] loss: 21.465\n",
      "[2, 27901] loss: 21.542\n",
      "[2, 28001] loss: 21.619\n",
      "[2, 28101] loss: 21.693\n",
      "[2, 28201] loss: 21.771\n",
      "[2, 28301] loss: 21.847\n",
      "[2, 28401] loss: 21.924\n",
      "[2, 28501] loss: 22.001\n",
      "[2, 28601] loss: 22.076\n",
      "[2, 28701] loss: 22.153\n",
      "[2, 28801] loss: 22.230\n",
      "[2, 28901] loss: 22.308\n",
      "[2, 29001] loss: 22.384\n",
      "[2, 29101] loss: 22.462\n",
      "[2, 29201] loss: 22.541\n",
      "[2, 29301] loss: 22.619\n",
      "[2, 29401] loss: 22.698\n",
      "[2, 29501] loss: 22.774\n",
      "[2, 29601] loss: 22.852\n",
      "[2, 29701] loss: 22.928\n",
      "[2, 29801] loss: 23.008\n",
      "[2, 29901] loss: 23.088\n",
      "[2, 30001] loss: 23.168\n",
      "[2, 30101] loss: 23.246\n",
      "[2, 30201] loss: 23.325\n",
      "[2, 30301] loss: 23.401\n",
      "[2, 30401] loss: 23.479\n",
      "[2, 30501] loss: 23.556\n",
      "[2, 30601] loss: 23.633\n",
      "[2, 30701] loss: 23.713\n",
      "[2, 30801] loss: 23.790\n",
      "[2, 30901] loss: 23.867\n",
      "[2, 31001] loss: 23.945\n",
      "[2, 31101] loss: 24.023\n",
      "[2, 31201] loss: 24.101\n",
      "[2, 31301] loss: 24.179\n",
      "[2, 31401] loss: 24.260\n",
      "[2, 31501] loss: 24.338\n",
      "[2, 31601] loss: 24.414\n",
      "[2, 31701] loss: 24.493\n",
      "[2, 31801] loss: 24.572\n",
      "[2, 31901] loss: 24.648\n",
      "[2, 32001] loss: 24.726\n",
      "[2, 32101] loss: 24.804\n",
      "[2, 32201] loss: 24.883\n",
      "[2, 32301] loss: 24.960\n",
      "[2, 32401] loss: 25.041\n",
      "[2, 32501] loss: 25.120\n",
      "[2, 32601] loss: 25.198\n",
      "[2, 32701] loss: 25.271\n",
      "[2, 32801] loss: 25.348\n",
      "[2, 32901] loss: 25.425\n",
      "[2, 33001] loss: 25.501\n",
      "[2, 33101] loss: 25.577\n",
      "[2, 33201] loss: 25.656\n",
      "[2, 33301] loss: 25.734\n",
      "[2, 33401] loss: 25.811\n",
      "[2, 33501] loss: 25.886\n",
      "[2, 33601] loss: 25.962\n",
      "[2, 33701] loss: 26.040\n",
      "[2, 33801] loss: 26.116\n",
      "[2, 33901] loss: 26.192\n",
      "[2, 34001] loss: 26.268\n",
      "[2, 34101] loss: 26.345\n",
      "[2, 34201] loss: 26.420\n",
      "[2, 34301] loss: 26.495\n",
      "[2, 34401] loss: 26.571\n",
      "[2, 34501] loss: 26.650\n",
      "[2, 34601] loss: 26.726\n",
      "[2, 34701] loss: 26.802\n",
      "[2, 34801] loss: 26.881\n",
      "[2, 34901] loss: 26.961\n",
      "[2, 35001] loss: 27.039\n",
      "[2, 35101] loss: 27.113\n",
      "[2, 35201] loss: 27.190\n",
      "[2, 35301] loss: 27.268\n",
      "[2, 35401] loss: 27.346\n",
      "[2, 35501] loss: 27.425\n",
      "[2, 35601] loss: 27.502\n",
      "[2, 35701] loss: 27.581\n",
      "[2, 35801] loss: 27.657\n",
      "[2, 35901] loss: 27.733\n",
      "[2, 36001] loss: 27.811\n",
      "[2, 36101] loss: 27.889\n",
      "[2, 36201] loss: 27.967\n",
      "[2, 36301] loss: 28.043\n",
      "[2, 36401] loss: 28.120\n",
      "[2, 36501] loss: 28.195\n",
      "[2, 36601] loss: 28.272\n",
      "[2, 36701] loss: 28.348\n",
      "[2, 36801] loss: 28.426\n",
      "[2, 36901] loss: 28.504\n",
      "[2, 37001] loss: 28.581\n",
      "[2, 37101] loss: 28.660\n",
      "[2, 37201] loss: 28.738\n",
      "[2, 37301] loss: 28.814\n",
      "[2, 37401] loss: 28.893\n",
      "[2, 37501] loss: 28.973\n",
      "[2, 37601] loss: 29.051\n",
      "[2, 37701] loss: 29.128\n",
      "[2, 37801] loss: 29.208\n",
      "[2, 37901] loss: 29.284\n",
      "[2, 38001] loss: 29.363\n",
      "[2, 38101] loss: 29.439\n",
      "[2, 38201] loss: 29.514\n",
      "[2, 38301] loss: 29.591\n",
      "[2, 38401] loss: 29.669\n",
      "[2, 38501] loss: 29.744\n",
      "[2, 38601] loss: 29.822\n",
      "[2, 38701] loss: 29.900\n",
      "[2, 38801] loss: 29.977\n",
      "[2, 38901] loss: 30.052\n",
      "[2, 39001] loss: 30.127\n",
      "[2, 39101] loss: 30.202\n",
      "[2, 39201] loss: 30.279\n",
      "[2, 39301] loss: 30.357\n",
      "[2, 39401] loss: 30.438\n",
      "[2, 39501] loss: 30.517\n",
      "[2, 39601] loss: 30.593\n",
      "[2, 39701] loss: 30.670\n",
      "[2, 39801] loss: 30.747\n",
      "[2, 39901] loss: 30.826\n",
      "[2, 40001] loss: 30.902\n",
      "[2, 40101] loss: 30.978\n",
      "[2, 40201] loss: 31.052\n",
      "[2, 40301] loss: 31.128\n",
      "[2, 40401] loss: 31.203\n",
      "[2, 40501] loss: 31.280\n",
      "[2, 40601] loss: 31.357\n",
      "[2, 40701] loss: 31.433\n",
      "[2, 40801] loss: 31.508\n",
      "[2, 40901] loss: 31.583\n",
      "[2, 41001] loss: 31.660\n",
      "[2, 41101] loss: 31.736\n",
      "[2, 41201] loss: 31.815\n",
      "[2, 41301] loss: 31.895\n",
      "[2, 41401] loss: 31.971\n",
      "[2, 41501] loss: 32.050\n",
      "[2, 41601] loss: 32.126\n",
      "[2, 41701] loss: 32.201\n",
      "[2, 41801] loss: 32.275\n",
      "[2, 41901] loss: 32.352\n",
      "[2, 42001] loss: 32.430\n",
      "[2, 42101] loss: 32.508\n",
      "[2, 42201] loss: 32.587\n",
      "[2, 42301] loss: 32.665\n",
      "[2, 42401] loss: 32.744\n",
      "[2, 42501] loss: 32.822\n",
      "[2, 42601] loss: 32.900\n",
      "[2, 42701] loss: 32.978\n",
      "[2, 42801] loss: 33.055\n",
      "[2, 42901] loss: 33.133\n",
      "[2, 43001] loss: 33.213\n",
      "[2, 43101] loss: 33.294\n",
      "[2, 43201] loss: 33.370\n",
      "[2, 43301] loss: 33.448\n",
      "[2, 43401] loss: 33.522\n",
      "[2, 43501] loss: 33.598\n",
      "[2, 43601] loss: 33.674\n",
      "[2, 43701] loss: 33.750\n",
      "[2, 43801] loss: 33.826\n",
      "[2, 43901] loss: 33.904\n",
      "[2, 44001] loss: 33.982\n",
      "[2, 44101] loss: 34.060\n",
      "[2, 44201] loss: 34.138\n",
      "[2, 44301] loss: 34.216\n",
      "[2, 44401] loss: 34.295\n",
      "[2, 44501] loss: 34.372\n",
      "[2, 44601] loss: 34.447\n",
      "[2, 44701] loss: 34.524\n",
      "[2, 44801] loss: 34.599\n",
      "[2, 44901] loss: 34.676\n",
      "[2, 45001] loss: 34.756\n",
      "[2, 45101] loss: 34.834\n",
      "[2, 45201] loss: 34.911\n",
      "[2, 45301] loss: 34.986\n",
      "[2, 45401] loss: 35.062\n",
      "[2, 45501] loss: 35.141\n",
      "[2, 45601] loss: 35.220\n",
      "[2, 45701] loss: 35.298\n",
      "[2, 45801] loss: 35.375\n",
      "[2, 45901] loss: 35.454\n",
      "[2, 46001] loss: 35.532\n",
      "[2, 46101] loss: 35.610\n",
      "[2, 46201] loss: 35.685\n",
      "[2, 46301] loss: 35.764\n",
      "[2, 46401] loss: 35.842\n",
      "[2, 46501] loss: 35.922\n",
      "[2, 46601] loss: 35.998\n",
      "[2, 46701] loss: 36.075\n",
      "[2, 46801] loss: 36.152\n",
      "[2, 46901] loss: 36.228\n",
      "[2, 47001] loss: 36.305\n",
      "[2, 47101] loss: 36.381\n",
      "[2, 47201] loss: 36.454\n",
      "[2, 47301] loss: 36.535\n",
      "[2, 47401] loss: 36.611\n",
      "[2, 47501] loss: 36.688\n",
      "[2, 47601] loss: 36.763\n",
      "[2, 47701] loss: 36.840\n",
      "[2, 47801] loss: 36.918\n",
      "[2, 47901] loss: 36.994\n",
      "[2, 48001] loss: 37.072\n",
      "[2, 48101] loss: 37.147\n",
      "[2, 48201] loss: 37.222\n",
      "[2, 48301] loss: 37.297\n",
      "[2, 48401] loss: 37.375\n",
      "[2, 48501] loss: 37.450\n",
      "[2, 48601] loss: 37.526\n",
      "[2, 48701] loss: 37.601\n",
      "[2, 48801] loss: 37.678\n",
      "[2, 48901] loss: 37.754\n",
      "[2, 49001] loss: 37.834\n",
      "[2, 49101] loss: 37.913\n",
      "[2, 49201] loss: 37.991\n",
      "[2, 49301] loss: 38.071\n",
      "[2, 49401] loss: 38.148\n",
      "[2, 49501] loss: 38.226\n",
      "[2, 49601] loss: 38.306\n",
      "[2, 49701] loss: 38.385\n",
      "[2, 49801] loss: 38.461\n",
      "[2, 49901] loss: 38.541\n",
      "[2, 50001] loss: 38.621\n",
      "[2, 50101] loss: 38.697\n",
      "[2, 50201] loss: 38.774\n",
      "[2, 50301] loss: 38.850\n",
      "[2, 50401] loss: 38.930\n",
      "[2, 50501] loss: 39.010\n",
      "[2, 50601] loss: 39.087\n",
      "[2, 50701] loss: 39.164\n",
      "[2, 50801] loss: 39.242\n",
      "[2, 50901] loss: 39.319\n",
      "[2, 51001] loss: 39.393\n",
      "[2, 51101] loss: 39.470\n",
      "[2, 51201] loss: 39.548\n",
      "[2, 51301] loss: 39.627\n",
      "[2, 51401] loss: 39.702\n",
      "[2, 51501] loss: 39.778\n",
      "[2, 51601] loss: 39.853\n",
      "[2, 51701] loss: 39.929\n",
      "[2, 51801] loss: 40.006\n",
      "[2, 51901] loss: 40.081\n",
      "[2, 52001] loss: 40.158\n",
      "[2, 52101] loss: 40.237\n",
      "[2, 52201] loss: 40.315\n",
      "[2, 52301] loss: 40.391\n",
      "[2, 52401] loss: 40.467\n",
      "[2, 52501] loss: 40.542\n",
      "[2, 52601] loss: 40.616\n",
      "[2, 52701] loss: 40.694\n",
      "[2, 52801] loss: 40.773\n",
      "[2, 52901] loss: 40.849\n",
      "[2, 53001] loss: 40.928\n",
      "[2, 53101] loss: 41.004\n",
      "[2, 53201] loss: 41.080\n",
      "[2, 53301] loss: 41.156\n",
      "[2, 53401] loss: 41.232\n",
      "[2, 53501] loss: 41.307\n",
      "[2, 53601] loss: 41.385\n",
      "[2, 53701] loss: 41.463\n",
      "[2, 53801] loss: 41.541\n",
      "[2, 53901] loss: 41.620\n",
      "[2, 54001] loss: 41.698\n",
      "[2, 54101] loss: 41.778\n",
      "[2, 54201] loss: 41.853\n",
      "[2, 54301] loss: 41.928\n",
      "[2, 54401] loss: 42.002\n",
      "[2, 54501] loss: 42.077\n",
      "[2, 54601] loss: 42.154\n",
      "[2, 54701] loss: 42.229\n",
      "[2, 54801] loss: 42.306\n",
      "[2, 54901] loss: 42.384\n",
      "[2, 55001] loss: 42.463\n",
      "[2, 55101] loss: 42.539\n",
      "[2, 55201] loss: 42.616\n",
      "[2, 55301] loss: 42.690\n",
      "[2, 55401] loss: 42.766\n",
      "[2, 55501] loss: 42.843\n",
      "[2, 55601] loss: 42.919\n",
      "[2, 55701] loss: 42.995\n",
      "[2, 55801] loss: 43.071\n",
      "[2, 55901] loss: 43.147\n",
      "[2, 56001] loss: 43.223\n",
      "[2, 56101] loss: 43.299\n",
      "[2, 56201] loss: 43.374\n",
      "[2, 56301] loss: 43.452\n",
      "[2, 56401] loss: 43.528\n",
      "[2, 56501] loss: 43.605\n",
      "[2, 56601] loss: 43.680\n",
      "[2, 56701] loss: 43.756\n",
      "[2, 56801] loss: 43.831\n",
      "[2, 56901] loss: 43.910\n",
      "[2, 57001] loss: 43.985\n",
      "[2, 57101] loss: 44.062\n",
      "[2, 57201] loss: 44.137\n",
      "[2, 57301] loss: 44.215\n",
      "[2, 57401] loss: 44.293\n",
      "[2, 57501] loss: 44.371\n",
      "[2, 57601] loss: 44.449\n",
      "[2, 57701] loss: 44.528\n",
      "[2, 57801] loss: 44.606\n",
      "[2, 57901] loss: 44.683\n",
      "[2, 58001] loss: 44.757\n",
      "[2, 58101] loss: 44.832\n",
      "[2, 58201] loss: 44.907\n",
      "[2, 58301] loss: 44.983\n",
      "[2, 58401] loss: 45.057\n",
      "[2, 58501] loss: 45.134\n",
      "[2, 58601] loss: 45.208\n",
      "[2, 58701] loss: 45.282\n",
      "[2, 58801] loss: 45.356\n",
      "[2, 58901] loss: 45.432\n",
      "[2, 59001] loss: 45.506\n",
      "[2, 59101] loss: 45.580\n",
      "[2, 59201] loss: 45.655\n",
      "[2, 59301] loss: 45.730\n",
      "[2, 59401] loss: 45.809\n",
      "[2, 59501] loss: 45.886\n",
      "[2, 59601] loss: 45.961\n",
      "[2, 59701] loss: 46.035\n",
      "[2, 59801] loss: 46.112\n",
      "[2, 59901] loss: 46.187\n",
      "[2, 60001] loss: 46.263\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Now train with classification loss\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        x = data[0].float().to(device)\n",
    "        label = data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        s, out = net(x)\n",
    "\n",
    "        loss = F.cross_entropy(out, label)\n",
    "        \n",
    "        # propagate back the gradients and update the model parameters/weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(f'[{epoch + 1}, {i:5d}] loss: {running_loss / 2000:.3f}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), PATH_AFTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  Seven Two   One   Zero \n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j].item()]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Model()\n",
    "net.load_state_dict(torch.load(PATH_BEFORE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, outputs = net(images[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "predicted = outputs.argmax(1)\n",
    "print(predicted[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 9 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        _, outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: Zero  is 0.0 %\n",
      "Accuracy for class: One   is 0.1 %\n",
      "Accuracy for class: Two   is 58.6 %\n",
      "Accuracy for class: Three is 0.2 %\n",
      "Accuracy for class: Four  is 20.1 %\n",
      "Accuracy for class: Five  is 10.1 %\n",
      "Accuracy for class: Six   is 2.4 %\n",
      "Accuracy for class: Seven is 0.7 %\n",
      "Accuracy for class: Eight is 1.1 %\n",
      "Accuracy for class: Nine  is 1.4 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        _, outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "06-image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

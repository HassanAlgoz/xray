{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable, Dict, Iterator\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch_directml\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "from utils import Params\n",
    "import model.net as net\n",
    "from evaluate import evaluate\n",
    "from data.reader import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'experiments/base_model'\n",
    "data_dir = 'data/small'\n",
    "restore_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    *,\n",
    "    device,\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim,\n",
    "    loss_fn: Callable,\n",
    "    data_iterator: Iterator,\n",
    "    metrics: Dict[str, Callable],\n",
    "    num_steps: int,\n",
    "    params: Params,\n",
    "):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        num_steps: (int) number of batches to train on\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        data = next(data_iterator, None)\n",
    "        if data == None:\n",
    "            break\n",
    "        input = data[0].to(device)\n",
    "        expected_output = data[1].to(device)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, expected_output)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output = output.data.cpu().numpy()\n",
    "            expected_output = expected_output.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary = {\n",
    "                metric: metrics[metric](output, expected_output) for metric in metrics\n",
    "            }\n",
    "            summary[\"loss\"] = loss.item()\n",
    "            summ.append(summary)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        t.set_postfix(loss=\"{:05.3f}\".format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: np.mean([x[metric] for x in summ]) for metric in metrics}\n",
    "    metrics_string = \" ; \".join(\n",
    "        \"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items()\n",
    "    )\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    *,\n",
    "    device,\n",
    "    model: torch.nn.Module,\n",
    "    train_data_loader: DataLoader,\n",
    "    val_data_loader: DataLoader,\n",
    "    optimizer: torch.optim,\n",
    "    loss_fn: Callable,\n",
    "    metrics: Dict[str, Callable],\n",
    "    params: Params,\n",
    "    model_dir: str,\n",
    "    restore_file: str = None,\n",
    ") -> None:\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(model_dir, restore_file + \".pth.tar\")\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        logging.info(f\"Epoch {epoch+1}/{params.num_epochs}\")\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        train(\n",
    "            device=device,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            data_iterator=train_data_loader,\n",
    "            metrics=metrics,\n",
    "            params=params,\n",
    "            num_steps=num_steps,\n",
    "        )\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_metrics = evaluate(\n",
    "            device=device,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            data_iterator=val_data_loader,\n",
    "            metrics=metrics,\n",
    "            params=params,\n",
    "            num_steps=num_steps,\n",
    "        )\n",
    "\n",
    "        val_acc = val_metrics[\"accuracy\"]\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optim_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            is_best=is_best,\n",
    "            checkpoint=model_dir,\n",
    "        )\n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 230\n",
    "# select the GPU device if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(seed)\n",
    "elif torch_directml.is_available():\n",
    "    device = torch_directml.device(torch_directml.default_device())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(model_dir, \"params.json\")\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_kv = KeyedVectors.load_word2vec_format(\n",
    "    \"./data/embeddings/glove.6B.100d.txt\", binary=False, no_header=True\n",
    ")\n",
    "embeddings = torch.tensor(embeddings_kv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30\n",
    "# load data\n",
    "train_dataset = Dataset(\"train\", data_dir, embeddings_kv, max_input_length)\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=params.batch_size, shuffle=True\n",
    ")\n",
    "params.train_size = len(train_dataset)\n",
    "\n",
    "val_dataset = Dataset(\"val\", data_dir, embeddings_kv, max_input_length)\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset, batch_size=params.batch_size, shuffle=True\n",
    ")\n",
    "params.val_size = len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.train_size, params.val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.Net(\n",
    "    device=device,\n",
    "    embeddings=embeddings,\n",
    "    num_heads=10,\n",
    "    num_layers=2,\n",
    "    num_classes=3,\n",
    "    input_window_size=max_input_length,\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "# fetch loss function and metrics\n",
    "loss_fn = net.loss_fn\n",
    "metrics = net.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    train_data_loader=iter(train_data_loader),\n",
    "    val_data_loader=iter(val_data_loader),\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    params=params,\n",
    "    model_dir=model_dir,\n",
    "    restore_file=restore_file,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "05-transformer-encoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
